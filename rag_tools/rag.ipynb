{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13cf20fa",
   "metadata": {},
   "source": [
    "Perfect 👌 now that you have a working **LLM module + prompting techniques**, the next big step is **RAG (Retrieval-Augmented Generation)**.\n",
    "\n",
    "Let’s structure this step by step:\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a pipeline where:\n",
    "\n",
    "1. You **store documents** in a vector database (or embeddings store).\n",
    "2. At query time, you **retrieve relevant chunks** of text based on semantic similarity.\n",
    "3. You **feed those retrieved chunks + user query** into your LLM to get an enriched answer.\n",
    "\n",
    "This avoids hallucinations and lets your model answer **domain-specific questions**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Minimal RAG Pipeline Components\n",
    "\n",
    "1. **Document Loader** → Load your text, PDFs, or dataset.\n",
    "2. **Text Splitter** → Break text into chunks (so retrieval works better).\n",
    "3. **Embedding Model** → Convert text chunks into dense vectors.\n",
    "4. **Vector Store** → Save and query embeddings (e.g., FAISS, Chroma).\n",
    "5. **Retriever** → Find top-k similar chunks for a query.\n",
    "6. **LLM Integration** → Combine retrieved chunks with user’s question, then send to your `LLMClient`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b2ed9",
   "metadata": {},
   "source": [
    "## 🔹 1. **Fixed-Size Chunking**\n",
    "\n",
    "* **Description**: Split text into chunks of fixed size (e.g., 500 tokens or 1000 characters).\n",
    "* **Pros**: Simple, fast, works for most use cases.\n",
    "* **Cons**: Can cut sentences in half, losing semantic meaning.\n",
    "* **Use case**: When text is uniform and doesn’t need sentence alignment (e.g., Wikipedia, transcripts).\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 2. **Sliding Window (Overlap) Chunking**\n",
    "\n",
    "* **Description**: Like fixed-size, but with overlap (e.g., 500 tokens with 100-token overlap).\n",
    "* **Pros**: Prevents loss of context at chunk boundaries.\n",
    "* **Cons**: More storage, redundant embeddings.\n",
    "* **Use case**: When answers may span across chunk boundaries (legal documents, long narratives).\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 3. **Sentence-Based Chunking**\n",
    "\n",
    "* **Description**: Split by sentences using NLP tools (spaCy, NLTK, etc.).\n",
    "* **Pros**: Keeps meaning intact, avoids cutting in the middle.\n",
    "* **Cons**: Some sentences can be too short/too long.\n",
    "* **Use case**: QA systems, chatbots where sentence coherence is key.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 4. **Semantic Chunking**\n",
    "\n",
    "* **Description**: Use embeddings to split text where topic/semantic shifts occur.\n",
    "* **Pros**: Preserves logical flow, more meaningful retrieval.\n",
    "* **Cons**: Expensive (needs embedding during chunking).\n",
    "* **Use case**: Research papers, mixed-topic articles.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 5. **Recursive Text Splitting (LangChain style)**\n",
    "\n",
    "* **Description**: Try to split by paragraph → if too long, split by sentence → if still too long, split by fixed-size.\n",
    "* **Pros**: Adaptive, balances context and size.\n",
    "* **Cons**: More complex, requires custom logic.\n",
    "* **Use case**: Generic, production-ready RAG pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 6. **Section / Structural Chunking**\n",
    "\n",
    "* **Description**: Split based on document structure (headings, sections, chapters).\n",
    "* **Pros**: Maintains context within logical units.\n",
    "* **Cons**: Requires structured docs (Markdown, PDF with headings).\n",
    "* **Use case**: Knowledge bases, manuals, legal docs.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 7. **Dialogue / Turn-Based Chunking**\n",
    "\n",
    "* **Description**: For chat logs, split by speaker turns.\n",
    "* **Pros**: Preserves conversational context.\n",
    "* **Cons**: Some turns may be too short.\n",
    "* **Use case**: Customer support RAG, call center analysis.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e95946",
   "metadata": {},
   "source": [
    "# 🔹 What are Embeddings?\n",
    "\n",
    "Embeddings are **numerical vector representations of text (or other data like images, audio, code, etc.)** in a high-dimensional space.\n",
    "\n",
    "* Words, sentences, or documents are mapped to vectors.\n",
    "* Similar meaning → vectors close together.\n",
    "* Different meaning → vectors far apart.\n",
    "\n",
    "👉 This is what makes **semantic search** in RAG possible. Instead of keyword search, embeddings allow \"meaning-based\" search.\n",
    "\n",
    "---\n",
    "\n",
    "# 🔹 Types of Embeddings (as per RAG)\n",
    "\n",
    "In RAG pipelines, you’ll generally encounter these embedding approaches:\n",
    "\n",
    "### 1. **Word Embeddings**\n",
    "\n",
    "* Classic ones like **Word2Vec, GloVe, FastText**.\n",
    "* Represent words only (no sentence-level context).\n",
    "* **Limitations**: Same vector for \"bank\" (river bank vs money bank).\n",
    "* ✅ Good for small/simple RAG but outdated.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Sentence / Document Embeddings**\n",
    "\n",
    "* Models: **Sentence-BERT (SBERT), Universal Sentence Encoder**.\n",
    "* Encode **whole sentences or paragraphs** into a single vector.\n",
    "* ✅ Most common in RAG because queries & chunks align well.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Contextual Embeddings (Transformer-based)**\n",
    "\n",
    "* From **BERT, RoBERTa, DistilBERT, DeBERTa**.\n",
    "* Tokens depend on surrounding context.\n",
    "* You can pool them (mean/max pooling) → single vector for sentence.\n",
    "* ✅ Good accuracy, moderate size.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Instruction-Tuned Embeddings**\n",
    "\n",
    "* Models: **OpenAI `text-embedding-ada-002`, `text-embedding-3-large/small`**, **E5, Instructor-XL**.\n",
    "* Specially trained for **retrieval + RAG tasks** (understand queries vs docs).\n",
    "* ✅ Best choice for production-grade RAG.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Multimodal Embeddings**\n",
    "\n",
    "* For **text + image + video**.\n",
    "* Models: **CLIP, BLIP, ALIGN**.\n",
    "* Example: Search “a dog with sunglasses” and retrieve the correct image.\n",
    "* ✅ Useful if your RAG involves **image search or video docs**.\n",
    "\n",
    "---\n",
    "\n",
    "# 🔹 Embedding Models (popular ones for RAG)\n",
    "\n",
    "| Model                      | Provider | Dim        | Speed  | Best Use                    |\n",
    "| -------------------------- | -------- | ---------- | ------ | --------------------------- |\n",
    "| **text-embedding-ada-002** | OpenAI   | 1536       | Fast   | General RAG                 |\n",
    "| **text-embedding-3-large** | OpenAI   | 3072       | Slower | High-accuracy RAG           |\n",
    "| **E5-base / E5-large**     | HF       | 768 / 1024 | Good   | Open-source RAG             |\n",
    "| **Instructor-XL**          | HF       | 768        | Good   | Query-aware RAG             |\n",
    "| **Sentence-BERT**          | HF       | 768        | Fast   | Lightweight semantic search |\n",
    "| **CLIP**                   | OpenAI   | 512–1024   | Good   | Image+text retrieval        |\n",
    "\n",
    "---\n",
    "\n",
    "# 🔹 Parameters to Know in Embedding Models\n",
    "\n",
    "1. **Dimension size** → higher = more expressive, but storage ↑\n",
    "\n",
    "   * e.g., SBERT = 768-d, OpenAI ada = 1536-d.\n",
    "2. **Cosine similarity / Dot product** → used for comparing embeddings.\n",
    "3. **Speed vs Accuracy tradeoff** → larger models (Instructor, E5-large) are slower but more accurate.\n",
    "4. **Specialization** → general (OpenAI) vs instruction-tuned (Instructor).\n",
    "\n",
    "---\n",
    "\n",
    "# 🔹 Embeddings in RAG Flow\n",
    "\n",
    "1. Chunk text (Fixed / Recursive).\n",
    "2. Encode each chunk → embedding.\n",
    "3. Store in **vector DB** (FAISS, Qdrant, Pinecone, Weaviate).\n",
    "4. Encode query → embedding.\n",
    "5. Find **nearest vectors** (semantic match).\n",
    "6. Pass retrieved docs → LLM for answer generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c6ef8",
   "metadata": {},
   "source": [
    "# 🔹 What is a Vector Database?\n",
    "\n",
    "A **vector database** stores high-dimensional vectors (embeddings) and provides **fast similarity search** (e.g., “which vectors are closest to my query?”).\n",
    "\n",
    "* Instead of SQL queries → we use **nearest neighbor search** (cosine similarity, dot product, L2 distance).\n",
    "* Example: You search “What is quantum computing?” → query gets embedded → vector DB finds most similar doc embeddings → send them to LLM for answering.\n",
    "\n",
    "---\n",
    "\n",
    "# 🔹 Core Features of Vector DBs\n",
    "\n",
    "1. **Vector storage** → Store embeddings from documents, images, audio, etc.\n",
    "2. **Indexing** → Organize vectors for **fast search** (e.g., HNSW, IVF, PQ).\n",
    "3. **Similarity search** → Cosine, dot product, Euclidean distance.\n",
    "4. **Metadata filtering** → e.g., only docs from `2023` or `category=finance`.\n",
    "5. **Scalability** → Handle millions/billions of vectors.\n",
    "6. **Integrations** → APIs for Python, REST, gRPC, LangChain, LlamaIndex.\n",
    "\n",
    "---\n",
    "\n",
    "# 🔹 Types of Vector Databases\n",
    "\n",
    "### 1. **Lightweight Local Indexes (Library-based)**\n",
    "\n",
    "* Examples: **FAISS (Meta), Annoy (Spotify), HNSWlib**.\n",
    "* Run locally (in-memory or disk).\n",
    "* Very fast for **small to medium datasets**.\n",
    "* ✅ Best for prototyping, small RAG apps.\n",
    "* ❌ No persistence or distributed scaling (unless wrapped).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Cloud-Native Managed Vector DBs**\n",
    "\n",
    "* Examples: **Pinecone, Weaviate Cloud, Qdrant Cloud, Milvus Cloud**.\n",
    "* Fully managed: auto-scaling, persistence, backups.\n",
    "* Built for **production RAG apps**.\n",
    "* ✅ Easy to integrate, powerful APIs.\n",
    "* ❌ Costly for large-scale usage.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Self-Hosted Vector DBs**\n",
    "\n",
    "* Examples: **Weaviate, Milvus, Qdrant (open source)**.\n",
    "* Run on your own server/cloud.\n",
    "* ✅ Scalable & production-ready with control.\n",
    "* ❌ Setup/maintenance required.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Hybrid Search Engines (Text + Vector)**\n",
    "\n",
    "* Examples: **Elasticsearch + vector plugin, Vespa, Redis Vector Search, PostgreSQL pgvector**.\n",
    "* Can handle **keyword + semantic search together**.\n",
    "* ✅ Useful when you need **hybrid RAG** (combine BM25 + embeddings).\n",
    "* ❌ Usually slower than purpose-built vector DBs.\n",
    "\n",
    "---\n",
    "\n",
    "# 🔹 Popular Vector DBs and Use Cases\n",
    "\n",
    "| Vector DB                              | Type                | Best Use Case                                |\n",
    "| -------------------------------------- | ------------------- | -------------------------------------------- |\n",
    "| **FAISS** (Meta)                       | Local library       | Prototyping, personal projects, academic RAG |\n",
    "| **Annoy** (Spotify)                    | Local library       | Music similarity, recommendation engines     |\n",
    "| **HNSWlib**                            | Local library       | Fast nearest neighbor, lightweight apps      |\n",
    "| **Pinecone**                           | Managed cloud       | Large-scale enterprise RAG                   |\n",
    "| **Qdrant**                             | Self-hosted / Cloud | Open-source, GPU-accelerated, scalable       |\n",
    "| **Weaviate**                           | Self-hosted / Cloud | Hybrid search, multimodal (text+image)       |\n",
    "| **Milvus**                             | Self-hosted / Cloud | Industry-scale big vector storage            |\n",
    "| **Elasticsearch (with vector search)** | Hybrid search       | Enterprise apps mixing keyword + semantic    |\n",
    "| **Redis Vector Search**                | In-memory + hybrid  | Real-time recommendations                    |\n",
    "| **Postgres pgvector**                  | SQL + Vector        | When you want SQL + embeddings in one DB     |\n",
    "\n",
    "---\n",
    "\n",
    "# 🔹 Use Cases of Vector DBs\n",
    "\n",
    "### 📚 Knowledge Retrieval (RAG)\n",
    "\n",
    "* Store embeddings of documents (PDFs, websites, manuals).\n",
    "* Query embeddings → retrieve relevant docs → answer with LLM.\n",
    "\n",
    "### 🎵 Recommendations\n",
    "\n",
    "* Store embeddings of songs → recommend similar tracks (Spotify style).\n",
    "\n",
    "### 🛍️ E-commerce Search\n",
    "\n",
    "* Store product embeddings → semantic search for “comfortable red running shoes”.\n",
    "\n",
    "### 🎥 Multimedia Search\n",
    "\n",
    "* Image/video embeddings (CLIP) → “find all images with a dog and a ball”.\n",
    "\n",
    "### 👨‍💻 Personalized Assistants\n",
    "\n",
    "* Store conversation history embeddings → retrieve context for chatbots.\n",
    "\n",
    "### 🧠 Anomaly Detection\n",
    "\n",
    "* Store user behavior embeddings → flag abnormal activity.\n",
    "\n",
    "---\n",
    "\n",
    "# 🔹 Key Differences in DB Choice\n",
    "\n",
    "* Small project → **FAISS** (fast, local).\n",
    "* Production with scaling → **Pinecone / Qdrant / Weaviate**.\n",
    "* SQL + vector together → **Postgres pgvector**.\n",
    "* Real-time apps → **Redis Vector Search**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf032da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K =3\n",
    "PDF_PATH = r\"C:\\Users\\SurajPatra\\Desktop\\FASTAPI\\GENAI_LIB\\rag_tools\\paracetamol.pdf\"\n",
    "EMBED_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "FAISS_INDEX_PATH = \"data/paracetamol_faiss.index\"\n",
    "CHUNKS_JSON = \"data/paracetamol_chunks.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f042061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import uuid\n",
    "import math\n",
    "import faiss\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec373542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SurajPatra\\Desktop\\FASTAPI\\GENAI_LIB\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# HTTP requests (for direct Ollama fallback)\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d20a92fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally import your LLMClient if packaged\n",
    "try:\n",
    "    from llm_tools.llm_tools import LLMConfig, LLMClient, Provider\n",
    "    HAS_LLM_CLIENT = True\n",
    "except Exception:\n",
    "    HAS_LLM_CLIENT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e311c3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama config (fallback direct HTTP)\n",
    "OLLAMA_BASE = os.environ.get(\"OLLAMA_BASE\", \"http://localhost:11434\")\n",
    "OLLAMA_MODEL = \"mistral:latest\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"rag-demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca5b3c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from all pages of a PDF using PyPDF2.\"\"\"\n",
    "    if not os.path.isfile(pdf_path):\n",
    "        raise FileNotFoundError(f\"PDF not found: {pdf_path}\")\n",
    "    text_parts = []\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            try:\n",
    "                txt = page.extract_text() or \"\"\n",
    "            except Exception as e:\n",
    "                logger.warning(\"Failed to extract page %s: %s\", i, e)\n",
    "                txt = \"\"\n",
    "            text_parts.append(txt)\n",
    "    full_text = \"\\n\".join(text_parts)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bb86530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Very simple sentence splitter using punctuation. Good enough for demo.\"\"\"\n",
    "    import re\n",
    "    # split on ., ?, ! followed by space and uppercase (rough)\n",
    "    sentences = re.split(r'(?<=[\\.\\?\\!])\\s+', text.strip())\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04ecc04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_sentences(sentences: List[str], max_tokens: int = 120, overlap: int = 20) -> List[str]:\n",
    "    \"\"\"\n",
    "    Build chunks by concatenating sentences until we reach approx max_tokens (estimated by words),\n",
    "    and include overlap sentences between chunks.\n",
    "    This is a simple heuristic chunker appropriate for short docs.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    cur = []\n",
    "    cur_words = 0\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        s = sentences[i]\n",
    "        s_words = len(s.split())\n",
    "        # if adding this sentence stays under limit, add\n",
    "        if cur_words + s_words <= max_tokens or not cur:\n",
    "            cur.append(s)\n",
    "            cur_words += s_words\n",
    "            i += 1\n",
    "        else:\n",
    "            chunk_text = \" \".join(cur)\n",
    "            chunks.append(chunk_text)\n",
    "            # overlap: roll back `overlap` words by keeping last few sentences\n",
    "            if overlap > 0:\n",
    "                # keep last n sentences (approx based on words)\n",
    "                keep = []\n",
    "                keep_words = 0\n",
    "                while cur and keep_words < overlap:\n",
    "                    sent = cur.pop()  # remove last sentence\n",
    "                    keep.insert(0, sent)\n",
    "                    keep_words += len(sent.split())\n",
    "                cur = keep\n",
    "                cur_words = sum(len(s.split()) for s in cur)\n",
    "            else:\n",
    "                cur = []\n",
    "                cur_words = 0\n",
    "    # flush\n",
    "    if cur:\n",
    "        chunks.append(\" \".join(cur))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d69d6d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFaissStore:\n",
    "    def __init__(self, dim: int):\n",
    "        self.dim = dim\n",
    "        self.index = faiss.IndexFlatIP(dim)  # use inner product; we'll normalize embeddings for cosine\n",
    "        self.id_to_meta = {}  # map index position -> metadata\n",
    "        self.ntotal = 0\n",
    "\n",
    "    def add(self, embeddings: np.ndarray, metas: List[dict]):\n",
    "        \"\"\"\n",
    "        embeddings: numpy array (N, dim)\n",
    "        metas: list of metadata dicts length N\n",
    "        \"\"\"\n",
    "        assert embeddings.shape[1] == self.dim\n",
    "        # normalize for cosine similarity with inner product\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        self.index.add(embeddings.astype(\"float32\"))\n",
    "        base = self.ntotal\n",
    "        for i, m in enumerate(metas):\n",
    "            self.id_to_meta[base + i] = m\n",
    "        self.ntotal += embeddings.shape[0]\n",
    "        logger.info(\"Added %d vectors, total now %d\", embeddings.shape[0], self.ntotal)\n",
    "\n",
    "    def search(self, query_vec: np.ndarray, k: int = 5) -> List[Tuple[float, dict]]:\n",
    "        # query_vec: (dim,) or (1,dim)\n",
    "        v = query_vec.reshape(1, -1).astype(\"float32\")\n",
    "        faiss.normalize_L2(v)\n",
    "        D, I = self.index.search(v, k)\n",
    "        results = []\n",
    "        for score, idx in zip(D[0], I[0]):\n",
    "            if idx < 0:\n",
    "                continue\n",
    "            meta = self.id_to_meta.get(int(idx), {})\n",
    "            results.append((float(score), meta))\n",
    "        return results\n",
    "    \n",
    "    def save(self, index_path: str, meta_path: str):\n",
    "        faiss.write_index(self.index, index_path)\n",
    "        with open(meta_path, \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump(self.id_to_meta, f, ensure_ascii=False, indent=2)\n",
    "        logger.info(\"Saved index to %s and metadata to %s\", index_path, meta_path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, index_path: str, meta_path: str):\n",
    "        if not os.path.isfile(index_path) or not os.path.isfile(meta_path):\n",
    "            raise FileNotFoundError(\"Index or metadata not found.\")\n",
    "        index = faiss.read_index(index_path)\n",
    "        with open(meta_path, \"r\", encoding=\"utf8\") as f:\n",
    "            id_to_meta = json.load(f)\n",
    "        dim = index.d\n",
    "        store = cls(dim)\n",
    "        store.index = index\n",
    "        store.id_to_meta = {int(k): v for k, v in id_to_meta.items()}\n",
    "        store.ntotal = index.ntotal\n",
    "        logger.info(\"Loaded index with dim=%d total=%d\", dim, store.ntotal)\n",
    "        return store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08360866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Ollama wrapper (if no LLMClient) ----------\n",
    "\n",
    "def ask_ollama_direct(prompt: str, model: str = OLLAMA_MODEL, base_url: str = OLLAMA_BASE, timeout: int = 60) -> str:\n",
    "    \"\"\"\n",
    "    Direct Ollama HTTP call to /api/chat (non-streaming).\n",
    "    Expects Ollama running locally at base_url.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = f\"{base_url}/api/chat\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"temperature\": 0.2}\n",
    "        }\n",
    "        resp = requests.post(url, json=payload, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        return data.get(\"message\", {}).get(\"content\", \"\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Ollama direct call failed: %s\", e)\n",
    "        return f\"[Ollama error] {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12ce3e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Main RAG flow ----------\n",
    "\n",
    "def build_index_from_pdf(pdf_path: str,\n",
    "                         embedder: SentenceTransformer,\n",
    "                         chunk_size_words: int = 120,\n",
    "                         overlap_words: int = 20,\n",
    "                         save_index: bool = True) -> SimpleFaissStore:\n",
    "    \"\"\"Extract, chunk, embed and build FAISS index. Returns SimpleFaissStore.\"\"\"\n",
    "    logger.info(\"Extracting text from PDF: %s\", pdf_path)\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    if not text.strip():\n",
    "        raise ValueError(\"No text extracted from PDF.\")\n",
    "\n",
    "    sentences = sentence_tokenize(text)\n",
    "    logger.info(\"Extracted %d sentences from PDF\", len(sentences))\n",
    "\n",
    "    chunks = chunk_sentences(sentences, max_tokens=chunk_size_words, overlap=overlap_words)\n",
    "    logger.info(\"Built %d chunks\", len(chunks))\n",
    "\n",
    "    # Build metadata list with ids and source info\n",
    "    metas = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        metas.append({\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"chunk_index\": i,\n",
    "            \"text\": chunk[:2000]  # cap for safety\n",
    "        })\n",
    "\n",
    "    # Compute embeddings in batches to avoid memory spikes\n",
    "    batch_size = 32\n",
    "    embeddings = []\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        emb = embedder.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n",
    "        embeddings.append(emb)\n",
    "        logger.info(\"Embedded batch %d-%d\", i, i + len(batch) - 1)\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    dim = embeddings.shape[1]\n",
    "    logger.info(\"Embeddings shape: %s\", embeddings.shape)\n",
    "\n",
    "    store = SimpleFaissStore(dim)\n",
    "    store.add(embeddings, metas)\n",
    "\n",
    "    if save_index:\n",
    "        store.save(FAISS_INDEX_PATH, CHUNKS_JSON)\n",
    "        logger.info(\"Saved index and chunk metadata for future runs.\")\n",
    "\n",
    "    return store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51eeebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query_with_rag(query: str, store: SimpleFaissStore, embedder: SentenceTransformer,\n",
    "                          llm_client=None, top_k: int = TOP_K) -> str:\n",
    "    \"\"\"Retrieve relevant chunks and ask LLM with a context prompt.\"\"\"\n",
    "    # Embed query\n",
    "    qvec = embedder.encode([query], convert_to_numpy=True)[0]\n",
    "    results = store.search(qvec, k=top_k)\n",
    "    logger.info(\"Retrieved %d results\", len(results))\n",
    "\n",
    "    # Build context text\n",
    "    context_pieces = []\n",
    "    for score, meta in results:\n",
    "        txt = meta.get(\"text\", \"\")\n",
    "        context_pieces.append(f\"(score:{score:.3f}) {txt}\")\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_pieces)\n",
    "\n",
    "    # Assemble prompt\n",
    "    prompt = (\n",
    "        \"You are an expert assistant. Use the following extracted context from a Paracetamol leaflet \"\n",
    "        \"to answer the question. If the context does not contain the answer, say you don't know.\\n\\n\"\n",
    "        f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query}\\n\\nPlease answer concisely and cite which context chunk you used.\"\n",
    "    )\n",
    "\n",
    "    logger.info(\"Prompt length: %d chars\", len(prompt))\n",
    "\n",
    "    # Ask through provided llm_client (preferred) or direct Ollama call\n",
    "    if llm_client is not None and HAS_LLM_CLIENT:\n",
    "        try:\n",
    "            res = llm_client.generate(user_prompt=prompt, system_prompt=None)\n",
    "            return res.text\n",
    "        except Exception as e:\n",
    "            logger.warning(\"LLMClient failed: %s. Falling back to direct Ollama.\", e)\n",
    "\n",
    "    # fallback: direct Ollama HTTP call\n",
    "    return ask_ollama_direct(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40ad9d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Demo runner ----------\n",
    "\n",
    "def run_demo(pdf_path=PDF_PATH):\n",
    "    # 1. load embedder\n",
    "    logger.info(\"Loading embedding model: %s\", EMBED_MODEL_NAME)\n",
    "    embedder = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "\n",
    "    # 2. build or load index\n",
    "    if os.path.exists(FAISS_INDEX_PATH) and os.path.exists(CHUNKS_JSON):\n",
    "        logger.info(\"Found saved index; loading...\")\n",
    "        store = SimpleFaissStore.load(FAISS_INDEX_PATH, CHUNKS_JSON)\n",
    "    else:\n",
    "        store = build_index_from_pdf(pdf_path, embedder, chunk_size_words=120, overlap_words=20, save_index=True)\n",
    "\n",
    "    # 3. prepare LLM client if available\n",
    "    llm_client = None\n",
    "    if HAS_LLM_CLIENT:\n",
    "        try:\n",
    "            cfg = LLMConfig(provider=Provider.OLLAMA, model=OLLAMA_MODEL, base_url=OLLAMA_BASE, temperature=0.2)\n",
    "            llm_client = LLMClient(cfg)\n",
    "            logger.info(\"LLMClient prepared (Ollama).\")\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Failed to init LLMClient: %s\", e)\n",
    "            llm_client = None\n",
    "\n",
    "    # 4. interactive queries\n",
    "    print(\"\\nRAG demo ready. Type a question about the Paracetamol leaflet (type 'exit' to quit).\")\n",
    "    while True:\n",
    "        q = input(\"\\nYour question > \").strip()\n",
    "        if q.lower() in (\"exit\", \"quit\"):\n",
    "            break\n",
    "        if not q:\n",
    "            continue\n",
    "        answer = answer_query_with_rag(q, store, embedder, llm_client=llm_client, top_k=TOP_K)\n",
    "        print(\"\\n=== Answer ===\\n\")\n",
    "        print(answer)\n",
    "        print(\"\\n==============\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64250b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rag-demo:Loading embedding model: all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:rag-demo:Extracting text from PDF: C:\\Users\\SurajPatra\\Desktop\\FASTAPI\\GENAI_LIB\\rag_tools\\paracetamol.pdf\n",
      "INFO:rag-demo:Extracted 9 sentences from PDF\n",
      "INFO:rag-demo:Built 3 chunks\n",
      "INFO:rag-demo:Embedded batch 0-2\n",
      "INFO:rag-demo:Embeddings shape: (3, 384)\n",
      "INFO:rag-demo:Added 3 vectors, total now 3\n",
      "INFO:rag-demo:Saved index to data/paracetamol_faiss.index and metadata to data/paracetamol_chunks.json\n",
      "INFO:rag-demo:Saved index and chunk metadata for future runs.\n",
      "INFO:rag-demo:LLMClient prepared (Ollama).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAG demo ready. Type a question about the Paracetamol leaflet (type 'exit' to quit).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 49.81it/s]\n",
      "INFO:rag-demo:Retrieved 3 results\n",
      "INFO:rag-demo:Prompt length: 2136 chars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Answer ===\n",
      "\n",
      " The other name of Paracetamol is acetaminophen. (Context: Paracetamol, also known as **acetaminophen**, is one of the most widely used medicines for relieving pain and reducing fever.)\n",
      "\n",
      "==============\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(PDF_PATH):\n",
    "        print(f\"Please put your paracetamol PDF at: {PDF_PATH} and re-run.\")\n",
    "        sys.exit(1)\n",
    "    run_demo(PDF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d838689d",
   "metadata": {},
   "source": [
    "# 🔹 Types of RAG & Their Use Cases\n",
    "\n",
    "### 1. **Vanilla RAG (Basic RAG)**\n",
    "\n",
    "* **How it works:**\n",
    "\n",
    "  * Query → Embed → Search chunks in vector DB → Send retrieved chunks + query to LLM → LLM answers.\n",
    "* **Strength:** Simple, easy to implement.\n",
    "* **Weakness:** Context window limited; only top-k chunks are passed.\n",
    "* **Use Cases:** FAQ bots, small document Q\\&A (like medical leaflets, company manuals).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **RAG with Re-Ranking**\n",
    "\n",
    "* **How it works:**\n",
    "\n",
    "  * Retrieve top-k chunks.\n",
    "  * Use a **cross-encoder re-ranker model** (like `ms-marco-MiniLM-L-12-v2`) to reorder results by semantic relevance.\n",
    "  * Pass top-n (better) chunks to LLM.\n",
    "* **Strength:** Much more accurate retrieval.\n",
    "* **Weakness:** Extra compute cost.\n",
    "* **Use Cases:** Legal/medical Q\\&A, enterprise knowledge search, where **precision matters**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Multi-Vector RAG**\n",
    "\n",
    "* **How it works:**\n",
    "\n",
    "  * Instead of one embedding per chunk, multiple embeddings are generated (e.g., one for entities, one for summary, one for keywords).\n",
    "  * Retrieval considers multiple “views” of the text.\n",
    "* **Strength:** Handles complex queries (synonyms, multi-perspective search).\n",
    "* **Weakness:** Larger storage, slower retrieval.\n",
    "* **Use Cases:** Scientific papers, financial reports, customer support KBs.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Hierarchical RAG (Tree-based / Map-Reduce RAG)**\n",
    "\n",
    "* **How it works:**\n",
    "\n",
    "  * Organize documents in hierarchy:\n",
    "\n",
    "    * First retrieve relevant sections (chapter-level).\n",
    "    * Then drill down (paragraph-level).\n",
    "  * Or use **map-reduce summarization** (summarize chunks → merge summaries → final answer).\n",
    "* **Strength:** Scales well to very large documents (books, multi-GB datasets).\n",
    "* **Weakness:** Slight delay due to multi-step reasoning.\n",
    "* **Use Cases:** E-discovery, research literature review, corporate compliance audits.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Agentic RAG**\n",
    "\n",
    "* **How it works:**\n",
    "\n",
    "  * LLM is treated as an **agent** with tools.\n",
    "  * It decides dynamically: “Should I query the vector DB? Which DB? Should I refine query?”\n",
    "  * Sometimes uses **multi-hop reasoning** (ask DB → rephrase → ask again).\n",
    "* **Strength:** Very flexible, interactive.\n",
    "* **Weakness:** Expensive (multiple LLM calls).\n",
    "* **Use Cases:** Customer support copilots, coding assistants, interactive chat with knowledge bases.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Fusion RAG (Query/Answer Fusion)**\n",
    "\n",
    "* **How it works:**\n",
    "\n",
    "  * Create multiple variations of the query (query expansion).\n",
    "  * Retrieve results for each.\n",
    "  * Fuse results before sending to LLM.\n",
    "* **Strength:** Handles vague/short queries well.\n",
    "* **Weakness:** Retrieval can bring noise.\n",
    "* **Use Cases:** Search engines, ecommerce product search, natural language business intelligence.\n",
    "\n",
    "---\n",
    "\n",
    "# 🎯 Summary\n",
    "\n",
    "* **Vanilla RAG** → Simple FAQs, small docs.\n",
    "* **Re-Ranking RAG** → High precision (law, medicine).\n",
    "* **Multi-Vector RAG** → Deep/complex queries.\n",
    "* **Hierarchical RAG** → Long books, enterprise data.\n",
    "* **Agentic RAG** → Copilots, tool-using assistants.\n",
    "* **Fusion RAG** → Search engines, vague queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9082d4eb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
